{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c77276e",
   "metadata": {},
   "source": [
    "# Defining Substorm Epoch\n",
    "In this notebook we will define substorm epoch for each substorm list creating new columns in for each substorm list in the AKR flimits file created in the Data_Download.ipynb notebook\n",
    "\n",
    "## Loading Frey 2004, Frey 2006 and Liou 2010 Substorm lists\n",
    "All three of these lists can be obtained from SuperMAG https://supermag.jhuapl.edu/products/ . However, currently the copy of Liou has bugs so the correct version should be obtained from https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2010JA015578&file=jgra20697-sup-0002-ds01.pdf.\n",
    "\n",
    "We assume the user has downloaded the Frey list from SuperMAG and the pdf containing the Liou list from the papers supplementary material.\n",
    "\n",
    "We have provided code to convert and format the pdf of the substorm list from Liou 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b02824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Wind_Waves.reading_tools import lioupdf2csv\n",
    "frey= pd.read_csv('../Example_Data/Full_Run_Through/frey.csv',\n",
    "                    parse_dates=['Date_UTC']) # Load Frey (SuperMAG already combines both Frey lists)\n",
    "liou= pd.read_csv(lioupdf2csv('../Example_Data/Full_Run_Through/liou.pdf'), parse_dates=['Date_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87084fd8",
   "metadata": {},
   "source": [
    "## Combine the Frey and Liou lists\n",
    "Now we must combine the FreyÂ and Liou lists into one combined imagery substorm lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229d8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "liou.rename(columns={'mlt':'MLT', 'mlat':'MLAT', 'glon':'GLON', 'glat':'GLAT'}, inplace=True)\n",
    "combined= pd.concat((liou, frey)).sort_values('Date_UTC')\n",
    "combined['Date_UTC']= combined['Date_UTC'].values.astype('datetime64[m]').astype('datetime64[ns]')\n",
    "combined.sort_values('Date_UTC', inplace=True)\n",
    "combined.index= list(range(len(combined)))\n",
    "combined.to_csv('../Example_Data/Full_Run_Through/combined_imagery.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83ec19",
   "metadata": {},
   "source": [
    "## Load Remaining Lists\n",
    "The remaining list Newell 2011, Ohtani 2020 and SOPHIE can all be obtained from SuperMAG (https://supermag.jhuapl.edu/products/). SuperMAG regularly updates these lists be rerunning the algorithms when new magnetometer data is uploaded and all these lists will cover the entire operational period of the SML index. We will assume the user has downloaded the lists from SuperMAG in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05b7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "newell= pd.read_csv('../Example_Data/Full_Run_Through/newell.csv',\n",
    "                    parse_dates=['Date_UTC'])\n",
    "ohtani= pd.read_csv('../Example_Data/Full_Run_Through/ohtani.csv',\n",
    "                    parse_dates=['Date_UTC'])\n",
    "sophie= pd.read_csv('../Example_Data/Full_Run_Through/sophie.csv',\n",
    "                    parse_dates=['Date_UTC'])\n",
    "combined= pd.read_csv('../Example_Data/Full_Run_Through/combined_imagery.csv', parse_dates=['Date_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2568ab",
   "metadata": {},
   "source": [
    "## Create Substorm Epoch\n",
    "First we reload the akr frequency limits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4145e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "akr_flims= pd.read_csv('../Example_Data/Full_Run_Through/akr_flims_working_version.csv', parse_dates=['Date_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ad0c9",
   "metadata": {},
   "source": [
    "Now we use the function find closest to obtain the times of substorm closest to the time of each sweep. We the take the difference to get substorm epoch. We also keep the MLT and MLat of the substorm onsets. Columns named combined, newell, ohtani and sophie contain the corresponding substorm epoch value in terms of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419ebeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatchSubpy.event_match import find_closest\n",
    "subs= [combined, newell, ohtani, sophie]\n",
    "sub_names= ['combined', 'newell', 'ohtani', 'sophie']\n",
    "for df, name in zip(subs, sub_names):\n",
    "    akr_flims[f'{name}_closest'], ind= find_closest(akr_flims.Date_UTC.values, df.Date_UTC.values, return_index=True)\n",
    "    akr_flims[f'{name}']= ((akr_flims.Date_UTC- akr_flims[f'{name}_closest']).values.\\\n",
    "        astype('timedelta64[ns]').astype('float64'))/(1e9*60)\n",
    "    akr_flims[f'{name}_MLT']=  df.MLT.values[ind]\n",
    "    akr_flims[f'{name}_MLAT']= df.MLAT.values[ind]\n",
    "akr_flims.to_csv('../Example_Data/Full_Run_Through/akr_substorm_epoch.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
